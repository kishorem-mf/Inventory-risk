{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca072ca9-7bc9-49a1-b055-add32e539b26",
   "metadata": {},
   "source": [
    "# Documentation: Data Processing and Transformation Notebook\n",
    "Overview\n",
    "\n",
    "This notebook provides a structured workflow for pre-processing data before loading it into SAP HANA Cloud. It automates the conversion, transformation, and preparation of data files into a format suitable for database ingestion.\n",
    "\n",
    "Key Features\n",
    "\n",
    "Excel to CSV Conversion\n",
    "\n",
    "CSV Processing\n",
    "\n",
    "Cleans and standardizes CSV content.\n",
    "\n",
    "Handles formatting issues (e.g., missing values, data type corrections, column name sanitization).\n",
    "\n",
    "Data Transformation\n",
    "\n",
    "Applies necessary transformations to align with SAP HANA Cloud schema requirements.\n",
    "\n",
    "Supports field renaming, type casting, and normalization steps.\n",
    "\n",
    "SQL Script Generation\n",
    "\n",
    "For each sheet in the original Excel file, the notebook generates:\n",
    "\n",
    "A CREATE TABLE SQL command.\n",
    "\n",
    "Schema definitions mapped from the transformed CSVs.\n",
    "\n",
    "Ensures that table creation scripts are consistent with HANA Cloud standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dce5b5-8f3e-4368-990f-60ff5800ff9c",
   "metadata": {},
   "source": [
    "# Excel to CSV Conversion\n",
    "\n",
    "Converts uploaded Excel spreadsheets into multiple CSV files (one per sheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cad49d2-55d0-4e17-8c04-444aa6d6afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def excel_to_csv(excel_file, output_dir=\"output_w30\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load Excel file\n",
    "    xls = pd.ExcelFile(excel_file)\n",
    "    sql_statements = {}\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        \n",
    "        # Save CSV\n",
    "        csv_file = os.path.join(output_dir, f\"{sheet_name}.csv\")\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "# Usage example\n",
    "excel_file_path = \"data v3/Master Data New - Updated.xlsx\"\n",
    "sql_output = excel_to_csv(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97269d06-cd05-40cc-9996-f13ac277dfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names: ['Product', 'Location', 'Customer Source', 'Location Product', 'Location Source', 'Production Source Header', 'Production Source Resource', 'Production Source Item']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your Excel file path\n",
    "excel_file = \"data v3/Master Data New - Updated.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "xls = pd.ExcelFile(excel_file)\n",
    "\n",
    "# Get sheet names\n",
    "sheet_names = xls.sheet_names\n",
    "\n",
    "print(\"Sheet names:\", sheet_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847d52e-0c2c-4d91-9ab5-9a5f1acd20eb",
   "metadata": {},
   "source": [
    "# Processing and generation of SQL commands for master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40348549-ac7d-4a21-a7d3-cd1114c57ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command for 'CURRENT_INVT.Product' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Location' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Customer Source' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Location Product' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Location Source' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Production Source Header' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Production Source Resource' written to 'sql_commands_master.sql'\n",
      "SQL command for 'CURRENT_INVT.Production Source Item' written to 'sql_commands_master.sql'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# sanitising column header names\n",
    "def clean_column_name(col):\n",
    "    # Remove unwanted characters and replace with underscore\n",
    "    col = re.sub(r'[\\s\\-\\[\\]\\(\\)\\\"/%*]', '_', col)\n",
    "    # Remove multiple underscores and strip leading/trailing ones\n",
    "    col = re.sub(r'_+', '_', col).strip('_')\n",
    "    return col\n",
    "\n",
    "def generate_sql_from_csv(csv_path, table_name=None, output_sql_file=\"sql_commands_master.sql\", schema_name=\"CURRENT_INVT\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # handling the missing/NULL values in tables by replacing them with 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    \n",
    "    # Clean column headers\n",
    "    original_columns = df.columns.tolist()\n",
    "    cleaned_columns = [clean_column_name(col) for col in original_columns]\n",
    "    df.columns = cleaned_columns  # Update DataFrame headers\n",
    "\n",
    "    # Use file name (without extension) as table name if not specified\n",
    "    if not table_name:\n",
    "        table_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "\n",
    "    # Start SQL command with schema\n",
    "    full_table_name = f\"{schema_name}.{table_name}\"\n",
    "    sql = f\"CREATE TABLE {full_table_name} (\\n\"\n",
    "    columns_sql = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Guess data type\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            col_type = \"INTEGER\"\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            col_type = \"FLOAT\"\n",
    "        elif pd.api.types.is_bool_dtype(df[col]):\n",
    "            col_type = \"BOOLEAN\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            col_type = \"DATETIME\"\n",
    "        else:\n",
    "            max_len = df[col].astype(str).map(len).max() or 255\n",
    "            col_type = f\"VARCHAR({min(max_len, 255)})\"\n",
    "\n",
    "        columns_sql.append(f\"    {col} {col_type}\")\n",
    "\n",
    "    sql += \",\\n\".join(columns_sql) + \"\\n);\\n\\n\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_sql_file, \"a\") as f:\n",
    "        f.write(sql)\n",
    "\n",
    "    print(f\"SQL command for '{full_table_name}' written to '{output_sql_file}'\")\n",
    "    return sql\n",
    "\n",
    "# Example usage\n",
    "csvs_name = ['Product', 'Location', 'Customer Source', 'Location Product', 'Location Source', 'Production Source Header', 'Production Source Resource', 'Production Source Item']\n",
    "for name in csvs_name:\n",
    "    csv_file_path = f\"output_w30/{name}.csv\"\n",
    "    sql_command = generate_sql_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e090684-de3e-478e-b3a2-9ef59220a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE \"CURRENT_INVT\".\"Product\";\n",
      "DROP TABLE \"CURRENT_INVT\".\"Location_Product\";\n",
      "DROP TABLE \"CURRENT_INVT\".\"Location_Source\";\n",
      "DROP TABLE \"CURRENT_INVT\".\"Production_Source_Header\";\n",
      "DROP TABLE \"CURRENT_INVT\".\"Production_Source_Resource\";\n",
      "DROP TABLE \"CURRENT_INVT\".\"Production_Source_Item\";\n",
      "Connection failed: (-10709, 'Connection failed (RTE:[89013] Socket closed by peer {192.168.0.111:63773 -> 18.156.196.160:443} (192.168.0.111:63773 -> cfe32093-429a-4e59-87dc-9f3e4da891bf.hna2.prod-eu10.hanacloud.ondemand.com:443))')\n"
     ]
    }
   ],
   "source": [
    "from hdbcli import dbapi\n",
    "\n",
    "# Define schema and table names\n",
    "schema_name = \"CURRENT_INVT\"\n",
    "csvs_name = [\n",
    "    'Product',\n",
    "    'Location_Product',\n",
    "    'Location_Source',\n",
    "    'Production_Source_Header',\n",
    "    'Production_Source_Resource',\n",
    "    'Production_Source_Item'\n",
    "]\n",
    "\n",
    "# Database connection parameters â€” replace with your values\n",
    "conn_params = {\n",
    "    \"address\": \"cfe32093-429a-4e59-87dc-9f3e4da891bf.hna2.prod-eu10.hanacloud.ondemand.com\",       # e.g., \"hostname.com\"\n",
    "    \"port\": 443,                     # default HANA port\n",
    "    \"user\": \"DBADMIN\",\n",
    "    \"password\": \"Bcone@1234567\"\n",
    "}\n",
    "\n",
    "# Generate DROP TABLE statements\n",
    "drop_statements = [f'DROP TABLE \"{schema_name}\".\"{table_name}\";' for table_name in csvs_name]\n",
    "\n",
    "# Print generated SQL (optional)\n",
    "for stmt in drop_statements:\n",
    "    print(stmt)\n",
    "\n",
    "# Connect to HANA and execute DROP TABLEs\n",
    "try:\n",
    "    conn = dbapi.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for stmt in drop_statements:\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            print(f\"Executed: {stmt}\")\n",
    "        except dbapi.Error as e:\n",
    "            print(f\"Error dropping table: {stmt}\\n{e}\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"All operations completed.\")\n",
    "\n",
    "except dbapi.Error as conn_err:\n",
    "    print(f\"Connection failed: {conn_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826e33a-f909-4b07-bfc4-202574d89ee3",
   "metadata": {},
   "source": [
    "# transposing the raw transactional data.\n",
    "\n",
    "this scripts transposes the transcational data. The resulting table has following column header\n",
    "IDs (product id, location id)\n",
    "Date (week year combination converted to date)\n",
    "key figures (e.g. demand, invertory target, project stock for Review DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8af59a4e-4c64-4c90-a256-553239d2d9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: output_w30/Demand Fulfillment.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Customer_ID', 'Customer_Priority']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Customer_ID: 100.0% non-null\n",
      "    Customer_Priority: 100.0% non-null\n",
      "  Key figures (5): ['Consensus Demand' 'Customer Receipts' 'Customer Demand Delivered Late']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "35   W31_2025  2025-07-28\n",
      "70   W32_2025  2025-08-04\n",
      "105  W33_2025  2025-08-11\n",
      "140  W34_2025  2025-08-18\n",
      "Rows before date filtering: 1820\n",
      "Rows after date filtering: 1820\n",
      "Week year conversion done\n",
      "Data available for pivoting: 1820 rows\n",
      "Sample melted data:\n",
      "   Product_ID          Customer_ID  Customer_Priority                      Key_Figure week_year  value        date\n",
      "0  FG-100-001  Customer Group 1000                  1                Consensus Demand  W30_2025  250.0  2025-07-21\n",
      "1  FG-100-001  Customer Group 1000                  1               Customer Receipts  W30_2025  250.0  2025-07-21\n",
      "2  FG-100-001  Customer Group 1000                  1  Customer Demand Delivered Late  W30_2025    0.0  2025-07-21\n",
      "Unique key figures: ['Consensus Demand' 'Customer Receipts' 'Customer Demand Delivered Late'\n",
      " 'Demand Fulfillment %' 'Average Delivery Delay of Customer Demand']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 1820\n",
      "  Customer_ID: 0 NaN values out of 1820\n",
      "  Customer_Priority: 0 NaN values out of 1820\n",
      "Using ID columns for pivot: ['Product_ID', 'Customer_ID', 'Customer_Priority']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (364, 9)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (35, 57)\n",
      "  Transformed shape: (364, 9)\n",
      "  Key figures found: ['Average Delivery Delay of Customer Demand', 'Consensus Demand', 'Customer Demand Delivered Late', 'Customer Receipts', 'Demand Fulfillment %']\n",
      "  Saved to: transformed_w30/Demand Fulfillment_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Review DC.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Location_ID']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Location_ID: 100.0% non-null\n",
      "  Key figures (8): ['Dependent Demand' 'Stock on Hand' 'Total Open PO+STO']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "80   W31_2025  2025-07-28\n",
      "160  W32_2025  2025-08-04\n",
      "240  W33_2025  2025-08-11\n",
      "320  W34_2025  2025-08-18\n",
      "Rows before date filtering: 4160\n",
      "Rows after date filtering: 4160\n",
      "Week year conversion done\n",
      "Data available for pivoting: 4160 rows\n",
      "Sample melted data:\n",
      "   Product_ID Location_ID         Key_Figure week_year  value        date\n",
      "0  FG-100-001      DC1000   Dependent Demand  W30_2025  250.0  2025-07-21\n",
      "1  FG-100-001      DC1000      Stock on Hand  W30_2025  800.0  2025-07-21\n",
      "2  FG-100-001      DC1000  Total Open PO+STO  W30_2025    0.0  2025-07-21\n",
      "Unique key figures: ['Dependent Demand' 'Stock on Hand' 'Total Open PO+STO'\n",
      " 'Incoming Transport Receipts' 'Outgoing Supply' 'Safety Stock (SOP)'\n",
      " 'Inventory Target' 'Projected Stock - Calculated']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 4160\n",
      "  Location_ID: 0 NaN values out of 4160\n",
      "Using ID columns for pivot: ['Product_ID', 'Location_ID']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (520, 11)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (80, 56)\n",
      "  Transformed shape: (520, 11)\n",
      "  Key figures found: ['Dependent Demand', 'Incoming Transport Receipts', 'Inventory Target', 'Outgoing Supply', 'Projected Stock - Calculated', 'Safety Stock (SOP)', 'Stock on Hand', 'Total Open PO+STO']\n",
      "  Saved to: transformed_w30/Review DC_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Review Component.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Location_ID']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Location_ID: 100.0% non-null\n",
      "  Key figures (9): ['Dependent Demand' 'Stock on Hand' 'Total Open PO+STO']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "54   W31_2025  2025-07-28\n",
      "108  W32_2025  2025-08-04\n",
      "162  W33_2025  2025-08-11\n",
      "216  W34_2025  2025-08-18\n",
      "Rows before date filtering: 2808\n",
      "Rows after date filtering: 2808\n",
      "Week year conversion done\n",
      "Data available for pivoting: 2808 rows\n",
      "Sample melted data:\n",
      "   Product_ID Location_ID         Key_Figure week_year    value        date\n",
      "0  RM-100-001      PL1000   Dependent Demand  W30_2025  18000.0  2025-07-21\n",
      "1  RM-100-001      PL1000      Stock on Hand  W30_2025      0.0  2025-07-21\n",
      "2  RM-100-001      PL1000  Total Open PO+STO  W30_2025      0.0  2025-07-21\n",
      "Unique key figures: ['Dependent Demand' 'Stock on Hand' 'Total Open PO+STO'\n",
      " 'Planned Production Receipts' 'Planned Transport Receipt'\n",
      " 'Incoming Transport Receipts' 'Safety Stock (SOP)' 'Inventory Target'\n",
      " 'Projected Stock - Calculated']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 2808\n",
      "  Location_ID: 0 NaN values out of 2808\n",
      "Using ID columns for pivot: ['Product_ID', 'Location_ID']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (312, 12)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (54, 56)\n",
      "  Transformed shape: (312, 12)\n",
      "  Key figures found: ['Dependent Demand', 'Incoming Transport Receipts', 'Inventory Target', 'Planned Production Receipts', 'Planned Transport Receipt', 'Projected Stock - Calculated', 'Safety Stock (SOP)', 'Stock on Hand', 'Total Open PO+STO']\n",
      "  Saved to: transformed_w30/Review Component_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Review Capacity.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Location_ID', 'Resource_ID2', 'Source_ID', 'Product_ID']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Location_ID: 100.0% non-null\n",
      "    Resource_ID2: 100.0% non-null\n",
      "    Source_ID: 100.0% non-null\n",
      "    Product_ID: 100.0% non-null\n",
      "  Key figures (3): ['Capacity Supply' 'Capacity Utilization'\n",
      " 'Capacity Usage of Production Resource']\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "   week_year        date\n",
      "0   W30_2025  2025-07-21\n",
      "23  W31_2025  2025-07-28\n",
      "46  W32_2025  2025-08-04\n",
      "69  W33_2025  2025-08-11\n",
      "92  W34_2025  2025-08-18\n",
      "Rows before date filtering: 1196\n",
      "Rows after date filtering: 1196\n",
      "Week year conversion done\n",
      "Data available for pivoting: 1196 rows\n",
      "Sample melted data:\n",
      "  Location_ID Resource_ID2             Source_ID  Product_ID                             Key_Figure week_year   value        date\n",
      "0      PL1000  RES-400-001                     0      __NULL                        Capacity Supply  W30_2025  2000.0  2025-07-21\n",
      "1      PL1000  RES-400-001                     0      __NULL                   Capacity Utilization  W30_2025     0.0  2025-07-21\n",
      "2      PL1000  RES-400-001  PL1000_FG400_001_PV1  FG-400-001  Capacity Usage of Production Resource  W30_2025     0.0  2025-07-21\n",
      "Unique key figures: ['Capacity Supply' 'Capacity Utilization'\n",
      " 'Capacity Usage of Production Resource']\n",
      "Checking ID columns for NaN values:\n",
      "  Location_ID: 0 NaN values out of 1196\n",
      "  Resource_ID2: 0 NaN values out of 1196\n",
      "  Source_ID: 0 NaN values out of 1196\n",
      "  Product_ID: 0 NaN values out of 1196\n",
      "Using ID columns for pivot: ['Location_ID', 'Resource_ID2', 'Source_ID', 'Product_ID']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (728, 8)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (23, 58)\n",
      "  Transformed shape: (728, 8)\n",
      "  Key figures found: ['Capacity Supply', 'Capacity Usage of Production Resource', 'Capacity Utilization']\n",
      "  Saved to: transformed_w30/Review Capacity_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Review Plant.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Location_ID']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Location_ID: 100.0% non-null\n",
      "  Key figures (9): ['Dependent Demand' 'Stock on Hand' 'Open Production Orders']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "107  W31_2025  2025-07-28\n",
      "214  W32_2025  2025-08-04\n",
      "321  W33_2025  2025-08-11\n",
      "428  W34_2025  2025-08-18\n",
      "Rows before date filtering: 5564\n",
      "Rows after date filtering: 5564\n",
      "Week year conversion done\n",
      "Data available for pivoting: 5564 rows\n",
      "Sample melted data:\n",
      "   Product_ID Location_ID              Key_Figure week_year   value        date\n",
      "0  FG-100-001      PL1000        Dependent Demand  W30_2025  3500.0  2025-07-21\n",
      "1  FG-100-001      PL1000           Stock on Hand  W30_2025     0.0  2025-07-21\n",
      "2  FG-100-001      PL1000  Open Production Orders  W30_2025     0.0  2025-07-21\n",
      "Unique key figures: ['Dependent Demand' 'Stock on Hand' 'Open Production Orders'\n",
      " 'Incoming Production Receipts' 'Outgoing Supply' 'Safety Stock (SOP)'\n",
      " 'Inventory Target' 'Projected Stock - Calculated' 0]\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 5564\n",
      "  Location_ID: 0 NaN values out of 5564\n",
      "Using ID columns for pivot: ['Product_ID', 'Location_ID']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (416, 12)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (107, 56)\n",
      "  Transformed shape: (416, 12)\n",
      "  Key figures found: [0, 'Dependent Demand', 'Incoming Production Receipts', 'Inventory Target', 'Open Production Orders', 'Outgoing Supply', 'Projected Stock - Calculated', 'Safety Stock (SOP)', 'Stock on Hand']\n",
      "  Saved to: transformed_w30/Review Plant_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Review Vendors.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Location_ID', 'Ship-To_Location_ID']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Location_ID: 100.0% non-null\n",
      "    Ship-To_Location_ID: 100.0% non-null\n",
      "  Key figures (5): ['Dependent Demand' 'Stock on Hand' 'Maximum External Receipt']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "39   W31_2025  2025-07-28\n",
      "78   W32_2025  2025-08-04\n",
      "117  W33_2025  2025-08-11\n",
      "156  W34_2025  2025-08-18\n",
      "Rows before date filtering: 2028\n",
      "Rows after date filtering: 2028\n",
      "Week year conversion done\n",
      "Data available for pivoting: 2028 rows\n",
      "Sample melted data:\n",
      "   Product_ID Location_ID Ship-To_Location_ID                Key_Figure week_year    value        date\n",
      "0  RM-100-001     VEN1000                   0          Dependent Demand  W30_2025  14300.0  2025-07-21\n",
      "1  RM-100-001     VEN1000                   0             Stock on Hand  W30_2025      0.0  2025-07-21\n",
      "2  RM-100-001     VEN1000                   0  Maximum External Receipt  W30_2025      0.0  2025-07-21\n",
      "Unique key figures: ['Dependent Demand' 'Stock on Hand' 'Maximum External Receipt'\n",
      " 'Unspecified Receipt/External Procurement' 'Outgoing Transport Supply']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 2028\n",
      "  Location_ID: 0 NaN values out of 2028\n",
      "  Ship-To_Location_ID: 0 NaN values out of 2028\n",
      "Using ID columns for pivot: ['Product_ID', 'Location_ID', 'Ship-To_Location_ID']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (936, 9)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (39, 57)\n",
      "  Transformed shape: (936, 9)\n",
      "  Key figures found: ['Dependent Demand', 'Maximum External Receipt', 'Outgoing Transport Supply', 'Stock on Hand', 'Unspecified Receipt/External Procurement']\n",
      "  Saved to: transformed_w30/Review Vendors_ts.csv\n",
      "\n",
      "============================================================\n",
      "Processing: output_w30/Profit Margin.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Customer_ID', 'Unnamed:_3']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Customer_ID: 100.0% non-null\n",
      "    Unnamed:_3: 100.0% non-null\n",
      "  Key figures (5): ['Customer Receipts' 'Planned Price' 'Constrained Demand Rev.']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Sample conversions:\n",
      "    week_year        date\n",
      "0    W30_2025  2025-07-21\n",
      "35   W31_2025  2025-07-28\n",
      "70   W32_2025  2025-08-04\n",
      "105  W33_2025  2025-08-11\n",
      "140  W34_2025  2025-08-18\n",
      "Rows before date filtering: 1820\n",
      "Rows after date filtering: 1820\n",
      "Week year conversion done\n",
      "Data available for pivoting: 1820 rows\n",
      "Sample melted data:\n",
      "   Product_ID Customer_ID  Unnamed:_3               Key_Figure week_year  value        date\n",
      "0  FG-100-001   CUST-1000         0.0        Customer Receipts  W30_2025  250.0  2025-07-21\n",
      "1  FG-100-001   CUST-1000         0.0            Planned Price  W30_2025    0.0  2025-07-21\n",
      "2  FG-100-001   CUST-1000         0.0  Constrained Demand Rev.  W30_2025    0.0  2025-07-21\n",
      "Unique key figures: ['Customer Receipts' 'Planned Price' 'Constrained Demand Rev.'\n",
      " 'Planned Cost Per Product' 'Constrained COGS']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 1820\n",
      "  Customer_ID: 0 NaN values out of 1820\n",
      "  Unnamed:_3: 0 NaN values out of 1820\n",
      "Using ID columns for pivot: ['Product_ID', 'Customer_ID', 'Unnamed:_3']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (364, 9)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (35, 57)\n",
      "  Transformed shape: (364, 9)\n",
      "  Key figures found: ['Constrained COGS', 'Constrained Demand Rev.', 'Customer Receipts', 'Planned Cost Per Product', 'Planned Price']\n",
      "  Saved to: transformed_w30/Profit Margin_ts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def get_week_start_date(week_year_str):\n",
    "    \"\"\"\n",
    "    Convert week-year format (e.g., 'w20 2025') to the first date of that week.\n",
    "    Assumes ISO week numbering where Monday is the first day of the week.\n",
    "    \"\"\"\n",
    "    if pd.isna(week_year_str):\n",
    "        return None\n",
    "        \n",
    "    week_year_str = str(week_year_str).strip()\n",
    "    \n",
    "    # Handle various formats: 'w20 2025', 'W20 2025', 'w20-2025', etc.\n",
    "    patterns = [\n",
    "        r'w(\\d+)[\\s\\-_]*(\\d{4})',  # w20 2025, w20-2025\n",
    "        r'(\\d{4})[\\s\\-_]*w(\\d+)',  # 2025 w20, 2025-w20\n",
    "        r'week[\\s\\-_]*(\\d+)[\\s\\-_]*(\\d{4})',  # week 20 2025\n",
    "        r'(\\d{4})[\\s\\-_]*week[\\s\\-_]*(\\d+)',  # 2025 week 20\n",
    "    ]\n",
    "    \n",
    "    week_num = None\n",
    "    year = None\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, week_year_str, re.IGNORECASE)\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            # Check which group is the year (4 digits) and which is week\n",
    "            if len(groups[0]) == 4:  # First group is year\n",
    "                year = int(groups[0])\n",
    "                week_num = int(groups[1])\n",
    "            else:  # Second group is year\n",
    "                week_num = int(groups[0])\n",
    "                year = int(groups[1])\n",
    "            break\n",
    "    \n",
    "    if week_num is None or year is None:\n",
    "        print(f\"Could not parse week format: '{week_year_str}'\")\n",
    "        return None\n",
    "    \n",
    "    # Validate week number\n",
    "    if week_num < 1 or week_num > 53:\n",
    "        print(f\"Invalid week number {week_num} in '{week_year_str}'\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get the first day of the year\n",
    "        jan_1 = datetime(year, 1, 1)\n",
    "        \n",
    "        # Find the first Monday of the year (ISO week 1)\n",
    "        if jan_1.weekday() <= 3:  # If Jan 1 is Mon-Thu, it's in week 1\n",
    "            first_monday = jan_1 - timedelta(days=jan_1.weekday())\n",
    "        else:  # If Jan 1 is Fri-Sun, week 1 starts next Monday\n",
    "            days_to_monday = 7 - jan_1.weekday()\n",
    "            first_monday = jan_1 + timedelta(days=days_to_monday)\n",
    "        \n",
    "        # Calculate the start date of the target week\n",
    "        target_date = first_monday + timedelta(weeks=week_num - 1)\n",
    "        \n",
    "        return target_date.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating date for '{week_year_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_columns(df):\n",
    "    \"\"\"\n",
    "    Automatically detect ID columns, key figure column, and week columns.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with 'id_columns', 'key_figure_column', 'week_columns', 'other_columns'\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Detect week columns (columns that match week patterns)\n",
    "    week_patterns = [\n",
    "        r'w\\d+[\\s\\-_]*\\d{4}',  # w20 2025, w20-2025, w20_2025\n",
    "        r'week[\\s\\-_]*\\d+[\\s\\-_]*\\d{4}',  # week 20 2025\n",
    "        r'\\d{4}[\\s\\-_]*w\\d+',  # 2025 w20, 2025-w20\n",
    "    ]\n",
    "    \n",
    "    week_columns = []\n",
    "    for col in columns:\n",
    "        for pattern in week_patterns:\n",
    "            if re.search(pattern, str(col), re.IGNORECASE):\n",
    "                week_columns.append(col)\n",
    "                break\n",
    "    \n",
    "    # Detect key figure column (likely contains categorical data)\n",
    "    key_figure_candidates = []\n",
    "    for col in columns:\n",
    "        if col.lower() in ['key figure', 'key_figure', 'metric', 'measure', 'indicator', 'type']:\n",
    "            key_figure_candidates.append(col)\n",
    "    \n",
    "    # If no obvious key figure column, look for columns with repeated categorical values\n",
    "    if not key_figure_candidates:\n",
    "        for col in columns:\n",
    "            if col not in week_columns:\n",
    "                unique_ratio = len(df[col].unique()) / len(df)\n",
    "                if unique_ratio < 0.1 and df[col].dtype == 'object':  # Less than 10% unique values\n",
    "                    key_figure_candidates.append(col)\n",
    "    \n",
    "    key_figure_column = key_figure_candidates[0] if key_figure_candidates else None\n",
    "    \n",
    "    # ID columns are remaining non-week, non-key-figure columns, but exclude numeric summary columns\n",
    "    excluded_columns = week_columns + ([key_figure_column] if key_figure_column else [])\n",
    "    potential_id_columns = [col for col in columns if col not in excluded_columns]\n",
    "    \n",
    "    # Filter out columns that are likely not ID columns (like 'Total', numeric summaries, etc.)\n",
    "    # Also filter out columns that are mostly NaN\n",
    "    id_columns = []\n",
    "    for col in potential_id_columns:\n",
    "        # Skip columns with names suggesting they're summary/total columns\n",
    "        if col.lower() in ['total', 'sum', 'average', 'avg', 'count', 'grand total']:\n",
    "            continue\n",
    "        \n",
    "        # Skip columns that are mostly NaN (more than 90% NaN)\n",
    "        if df[col].isna().sum() / len(df) > 0.9:\n",
    "            continue\n",
    "            \n",
    "        id_columns.append(col)\n",
    "    \n",
    "    # Other columns (like 'total' etc.)\n",
    "    other_columns = [col for col in potential_id_columns if col not in id_columns]\n",
    "    \n",
    "    return {\n",
    "        'id_columns': id_columns,\n",
    "        'key_figure_column': key_figure_column,\n",
    "        'week_columns': week_columns,\n",
    "        'other_columns': other_columns\n",
    "    }\n",
    "\n",
    "def transform_single_csv(input_file, output_file, config=None):\n",
    "    \"\"\"\n",
    "    Transform a single CSV from wide format to long format.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input CSV file\n",
    "        output_file (str): Path to output CSV file (optional)\n",
    "        config (dict): Manual configuration for columns (optional)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {input_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "    # Step 1: Replace string \"(None)\" with empty string\n",
    "    df.replace(to_replace=r\"\\(None\\)\", value=\"\", regex=True, inplace=True)\n",
    "    \n",
    "    # Step 2: Replace blank strings and NaN with 0\n",
    "    df.replace(to_replace=[\"\", np.nan], value=0, inplace=True)\n",
    "    \n",
    "    # Use provided config or detect columns automatically\n",
    "    if config:\n",
    "        column_info = config\n",
    "    else:\n",
    "        column_info = detect_columns(df)\n",
    "    \n",
    "    print(f\"Detected columns:\")\n",
    "    print(f\"  ID columns: {column_info['id_columns']}\")\n",
    "    print(f\"  Key figure column: {column_info['key_figure_column']}\")\n",
    "    print(f\"  Week columns: {len(column_info['week_columns'])} columns\")\n",
    "    print(f\"  Other columns: {column_info['other_columns']}\")\n",
    "    \n",
    "    # Show which ID columns have data\n",
    "    if column_info['id_columns']:\n",
    "        print(f\"  ID column data availability:\")\n",
    "        for col in column_info['id_columns']:\n",
    "            non_null_pct = (1 - df[col].isna().sum() / len(df)) * 100\n",
    "            print(f\"    {col}: {non_null_pct:.1f}% non-null\")\n",
    "    \n",
    "    # Show sample key figures\n",
    "    if column_info['key_figure_column']:\n",
    "        unique_kf = df[column_info['key_figure_column']].unique()\n",
    "        print(f\"  Key figures ({len(unique_kf)}): {unique_kf[:3]}{'...' if len(unique_kf) > 3 else ''}\")\n",
    "    \n",
    "    # Validate detection\n",
    "    if not column_info['week_columns']:\n",
    "        print(\"ERROR: No week columns detected!\")\n",
    "        return None\n",
    "    \n",
    "    if not column_info['key_figure_column']:\n",
    "        print(\"WARNING: No key figure column detected. Proceeding without pivoting key figures.\")\n",
    "        return transform_without_key_figures(df, column_info, output_file)\n",
    "    \n",
    "    # Prepare for melting\n",
    "    id_vars = column_info['id_columns'] + [column_info['key_figure_column']]\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    try:\n",
    "        melted_df = pd.melt(\n",
    "            df,\n",
    "            id_vars=id_vars,\n",
    "            value_vars=column_info['week_columns'],\n",
    "            var_name='week_year',\n",
    "            value_name='value'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during melting: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert week_year to dates\n",
    "    print(\"Converting week formats to dates...\")\n",
    "    \n",
    "    # Debug: Show sample week formats\n",
    "    sample_weeks = melted_df['week_year'].unique()[:5]\n",
    "    print(f\"Sample week formats: {sample_weeks}\")\n",
    "    \n",
    "    melted_df['date'] = melted_df['week_year'].apply(get_week_start_date)\n",
    "    \n",
    "    # Debug: Show conversion results\n",
    "    sample_conversions = melted_df[['week_year', 'date']].drop_duplicates().head(5)\n",
    "    print(\"Sample conversions:\")\n",
    "    print(sample_conversions.to_string())\n",
    "    \n",
    "    # Remove rows where date conversion failed\n",
    "    before_count = len(melted_df)\n",
    "    melted_df = melted_df.dropna(subset=['date'])\n",
    "    after_count = len(melted_df)\n",
    "    print(f\"Rows before date filtering: {before_count}\")\n",
    "    print(f\"Rows after date filtering: {after_count}\")\n",
    "    if before_count != after_count:\n",
    "        print(f\"WARNING: {before_count - after_count} rows dropped due to date conversion issues\")\n",
    "        \n",
    "        # Show failed conversions\n",
    "        failed_weeks = df.columns[df.columns.str.contains('w', case=False, na=False)][:3]\n",
    "        print(f\"Sample failed week formats: {failed_weeks}\")\n",
    "    \n",
    "    print(\"Week year conversion done\")\n",
    "    \n",
    "    # Debug: Check data before pivoting\n",
    "    print(f\"Data available for pivoting: {len(melted_df)} rows\")\n",
    "    if len(melted_df) > 0:\n",
    "        print(\"Sample melted data:\")\n",
    "        print(melted_df.head(3).to_string())\n",
    "        print(f\"Unique key figures: {melted_df[column_info['key_figure_column']].unique()}\")\n",
    "        \n",
    "        # Check for NaN values in ID columns\n",
    "        print(\"Checking ID columns for NaN values:\")\n",
    "        for col in column_info['id_columns']:\n",
    "            nan_count = melted_df[col].isna().sum()\n",
    "            print(f\"  {col}: {nan_count} NaN values out of {len(melted_df)}\")\n",
    "        \n",
    "        # If too many NaN values, filter them out or handle differently\n",
    "        id_cols_with_data = [col for col in column_info['id_columns'] \n",
    "                           if melted_df[col].isna().sum() < len(melted_df) * 0.9]\n",
    "        \n",
    "        if not id_cols_with_data:\n",
    "            print(\"WARNING: All ID columns are mostly NaN. Using index-based grouping.\")\n",
    "            # Add a row identifier to prevent empty pivot\n",
    "            melted_df['row_id'] = melted_df.index // len(column_info['week_columns'])\n",
    "            id_cols_with_data = ['row_id']\n",
    "        \n",
    "        print(f\"Using ID columns for pivot: {id_cols_with_data}\")\n",
    "    else:\n",
    "        print(\"ERROR: No data available for pivoting!\")\n",
    "        return None\n",
    "    \n",
    "    # Pivot key figures to columns\n",
    "    try:\n",
    "        print(\"key figure cols\",column_info['key_figure_column'])\n",
    "        pivot_df = melted_df.pivot_table(\n",
    "            index=id_cols_with_data + ['date'],\n",
    "            columns=column_info['key_figure_column'],\n",
    "            values='value',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        pivot_df.columns.name = None\n",
    "        \n",
    "        print(f\"Pivot successful. Shape: {pivot_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during pivoting: {e}\")\n",
    "        print(\"Attempting alternative approach...\")\n",
    "        \n",
    "        # Alternative: Don't pivot, just keep as long format\n",
    "        result_df = melted_df.copy()\n",
    "        result_df = result_df.sort_values(id_cols_with_data + ['date'])\n",
    "        return result_df\n",
    "    \n",
    "    # Sort the result\n",
    "    sort_columns = id_cols_with_data + ['date']\n",
    "    pivot_df = pivot_df.sort_values(sort_columns)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTransformation Summary:\")\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    print(f\"  Transformed shape: {pivot_df.shape}\")\n",
    "    print(f\"  Key figures found: {[col for col in pivot_df.columns if col not in id_cols_with_data + ['date']]}\")\n",
    "    \n",
    "    # Save if output file specified\n",
    "    if output_file:\n",
    "        try:\n",
    "            pivot_df.to_csv(output_file, index=False)\n",
    "            print(f\"  Saved to: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file: {e}\")\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "def transform_without_key_figures(df, column_info, output_file=None):\n",
    "    \"\"\"\n",
    "    Transform CSV when no key figure column is detected (simple unpivot).\n",
    "    \"\"\"\n",
    "    print(\"Performing simple unpivot transformation...\")\n",
    "    \n",
    "    melted_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=column_info['id_columns'],\n",
    "        value_vars=column_info['week_columns'],\n",
    "        var_name='week_year',\n",
    "        value_name='value'\n",
    "    )\n",
    "    \n",
    "    melted_df['date'] = melted_df['week_year'].apply(get_week_start_date)\n",
    "    melted_df = melted_df.dropna(subset=['date'])\n",
    "    melted_df = melted_df.drop('week_year', axis=1)\n",
    "    \n",
    "    sort_columns = column_info['id_columns'] + ['date']\n",
    "    melted_df = melted_df.sort_values(sort_columns)\n",
    "    \n",
    "    if output_file:\n",
    "        melted_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    return melted_df\n",
    "\n",
    "def transform_multiple_csv(input_folder, output_folder=None, file_pattern=\"*.csv\"):\n",
    "    \"\"\"\n",
    "    Transform multiple CSV files in a folder.\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str): Path to folder containing input CSV files\n",
    "        output_folder (str): Path to folder for output files (optional)\n",
    "        file_pattern (str): Pattern to match files (default: \"*.csv\")\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of transformed dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    input_path = Path(input_folder)\n",
    "    if not input_path.exists():\n",
    "        print(f\"Input folder {input_folder} does not exist!\")\n",
    "        return {}\n",
    "    \n",
    "    # Create output folder if specified\n",
    "    if output_folder:\n",
    "        output_path = Path(output_folder)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all matching files\n",
    "    csv_files = list(input_path.glob(file_pattern))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No files found matching pattern {file_pattern} in {input_folder}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def preview_file_structure(input_file, num_rows=3):\n",
    "    \"\"\"\n",
    "    Preview file structure to understand the data before transformation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FILE STRUCTURE PREVIEW: {input_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        column_info = detect_columns(df)\n",
    "        print(f\"\\nDetected Structure:\")\n",
    "        print(f\"  ID columns: {column_info['id_columns']}\")\n",
    "        print(f\"  Key figure column: {column_info['key_figure_column']}\")\n",
    "        print(f\"  Week columns: {len(column_info['week_columns'])} columns\")\n",
    "        print(f\"  Sample week columns: {column_info['week_columns'][:5]}\")\n",
    "        \n",
    "        if column_info['key_figure_column']:\n",
    "            print(f\"  Unique key figures: {df[column_info['key_figure_column']].unique()}\")\n",
    "        \n",
    "        print(f\"\\nFirst {num_rows} rows:\")\n",
    "        print(df.head(num_rows).to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing {input_file}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage and configuration\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Option 1: Transform a single file\n",
    "    # transform_single_csv(\"output_w29/Demand Fulfillment.csv\", \"transformed_output_w29/Demand_Fulfillment_ts.csv\")\n",
    "    csvs_name = ['Demand Fulfillment','Review DC', 'Review Component', 'Review Capacity', 'Review Plant', 'Review Vendors','Profit Margin' ]\n",
    "\n",
    "    for name in csvs_name:\n",
    "    # Option 1: Transform a single file\n",
    "        transform_single_csv(f\"output_w30/{name}.cszv\", f\"transformed_w30/{name}_ts.csv\")\n",
    "    #name = 'Profit Margin'\n",
    "    #transform_single_csv(f\"output_w30/{name}.csv\", f\"ts_w230_dates/{name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7021cc0e-833c-4ae3-b4b6-275b9d27ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated SQL written to: sql_commands_history_table.sql\n",
      "Updated SQL written to: sql_commands_history_table.sql\n",
      "Updated SQL written to: sql_commands_history_table.sql\n",
      "Updated SQL written to: sql_commands_history_table.sql\n",
      "Updated SQL written to: sql_commands_history_table.sql\n",
      "Updated SQL written to: sql_commands_history_table.sql\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_schema_to_create_table(input_file, output_file=None, schema='invt_historical_data'):\n",
    "    with open(input_file, 'r') as f:\n",
    "        sql = f.read()\n",
    "\n",
    "    # Add schema before table name in CREATE TABLE statements\n",
    "    modified_sql = re.sub(\n",
    "        r'(?i)(CREATE\\s+TABLE\\s+)(\\w+)', \n",
    "        rf'\\1{schema}.\\2', \n",
    "        sql\n",
    "    )\n",
    "\n",
    "    # Write output\n",
    "    with open(output_file or input_file, 'w') as f:\n",
    "        f.write(modified_sql)\n",
    "\n",
    "    print(f\"Updated SQL written to: {output_file or input_file}\")\n",
    "\n",
    "# Example usage\n",
    "csvs_name = ['Demand Fulfillment_prev','Review DC_prev', 'Review Component_prev', 'Review Capacity_prev', 'Review Plant_prev', 'Review Vendors_prev' ]\n",
    "\n",
    "for name in csvs_name:\n",
    "    add_schema_to_create_table(f\"previous_files_w29_w30/{name}.csv\",\"sql_commands_history_table.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc626625-0e5a-4653-aa50-5f7f85e6a8bf",
   "metadata": {},
   "source": [
    "# updating the above tranformaiton script for adding refreshed date for storing historical data\n",
    "\n",
    "historical data : schema INVT_HISTORICAL_DATA containing W29 and W30 transaction data. to be referred for scenarios where demand forecasting analysis is needed\n",
    "\n",
    "<br>\n",
    "check scenario 11, 12 in the Scenario Analysis Reference sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6551a74c-1bc0-4cbd-ab17-6182668cc2a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: output_w30/Profit Margin.csv\n",
      "============================================================\n",
      "Detected columns:\n",
      "  ID columns: ['Product_ID', 'Customer_ID', 'Unnamed:_3']\n",
      "  Key figure column: Key_Figure\n",
      "  Week columns: 52 columns\n",
      "  Other columns: ['Total']\n",
      "  ID column data availability:\n",
      "    Product_ID: 100.0% non-null\n",
      "    Customer_ID: 100.0% non-null\n",
      "    Unnamed:_3: 100.0% non-null\n",
      "  Key figures (5): ['Customer Receipts' 'Planned Price' 'Constrained Demand Rev.']...\n",
      "Converting week formats to dates...\n",
      "Sample week formats: ['W30_2025' 'W31_2025' 'W32_2025' 'W33_2025' 'W34_2025']\n",
      "Rows before date filtering: 1820\n",
      "Rows after date filtering: 1820\n",
      "Week year conversion done\n",
      "Data available for pivoting: 1820 rows\n",
      "Sample melted data:\n",
      "   Product_ID Customer_ID  Unnamed:_3               Key_Figure week_year  value        date\n",
      "0  FG-100-001   CUST-1000         0.0        Customer Receipts  W30_2025  250.0  2025-07-21\n",
      "1  FG-100-001   CUST-1000         0.0            Planned Price  W30_2025    0.0  2025-07-21\n",
      "2  FG-100-001   CUST-1000         0.0  Constrained Demand Rev.  W30_2025    0.0  2025-07-21\n",
      "Unique key figures: ['Customer Receipts' 'Planned Price' 'Constrained Demand Rev.'\n",
      " 'Planned Cost Per Product' 'Constrained COGS']\n",
      "Checking ID columns for NaN values:\n",
      "  Product_ID: 0 NaN values out of 1820\n",
      "  Customer_ID: 0 NaN values out of 1820\n",
      "  Unnamed:_3: 0 NaN values out of 1820\n",
      "Using ID columns for pivot: ['Product_ID', 'Customer_ID', 'Unnamed:_3']\n",
      "key figure cols Key_Figure\n",
      "Pivot successful. Shape: (364, 9)\n",
      "\n",
      "Transformation Summary:\n",
      "  Original shape: (35, 57)\n",
      "  Transformed shape: (364, 9)\n",
      "  Key figures found: ['Constrained COGS', 'Constrained Demand Rev.', 'Customer Receipts', 'Planned Cost Per Product', 'Planned Price']\n",
      "  Created new CSV: previous_files_w29_w30/Profit Margin_prev_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_week_start_date(week_year_str):\n",
    "    \"\"\"\n",
    "    Convert week-year format (e.g., 'w20 2025') to the first date of that week.\n",
    "    Assumes ISO week numbering where Monday is the first day of the week.\n",
    "    \"\"\"\n",
    "    if pd.isna(week_year_str):\n",
    "        return None\n",
    "        \n",
    "    week_year_str = str(week_year_str).strip()\n",
    "    \n",
    "    # Handle various formats: 'w20 2025', 'W20 2025', 'w20-2025', etc.\n",
    "    patterns = [\n",
    "        r'w(\\d+)[\\s\\-_]*(\\d{4})',  # w20 2025, w20-2025\n",
    "        r'(\\d{4})[\\s\\-_]*w(\\d+)',  # 2025 w20, 2025-w20\n",
    "        r'week[\\s\\-_]*(\\d+)[\\s\\-_]*(\\d{4})',  # week 20 2025\n",
    "        r'(\\d{4})[\\s\\-_]*week[\\s\\-_]*(\\d+)',  # 2025 week 20\n",
    "    ]\n",
    "    \n",
    "    week_num = None\n",
    "    year = None\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, week_year_str, re.IGNORECASE)\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            # Check which group is the year (4 digits) and which is week\n",
    "            if len(groups[0]) == 4:  # First group is year\n",
    "                year = int(groups[0])\n",
    "                week_num = int(groups[1])\n",
    "            else:  # Second group is year\n",
    "                week_num = int(groups[0])\n",
    "                year = int(groups[1])\n",
    "            break\n",
    "    \n",
    "    if week_num is None or year is None:\n",
    "        print(f\"Could not parse week format: '{week_year_str}'\")\n",
    "        return None\n",
    "    \n",
    "    # Validate week number\n",
    "    if week_num < 1 or week_num > 53:\n",
    "        print(f\"Invalid week number {week_num} in '{week_year_str}'\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get the first day of the year\n",
    "        jan_1 = datetime(year, 1, 1)\n",
    "        \n",
    "        # Find the first Monday of the year (ISO week 1)\n",
    "        if jan_1.weekday() <= 3:  # If Jan 1 is Mon-Thu, it's in week 1\n",
    "            first_monday = jan_1 - timedelta(days=jan_1.weekday())\n",
    "        else:  # If Jan 1 is Fri-Sun, week 1 starts next Monday\n",
    "            days_to_monday = 7 - jan_1.weekday()\n",
    "            first_monday = jan_1 + timedelta(days=days_to_monday)\n",
    "        \n",
    "        # Calculate the start date of the target week\n",
    "        target_date = first_monday + timedelta(weeks=week_num - 1)\n",
    "        \n",
    "        return target_date.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating date for '{week_year_str}': {e}\")\n",
    "        return None\n",
    "        \n",
    "def detect_columns(df):\n",
    "    \"\"\"\n",
    "    Automatically detect ID columns, key figure column, and week columns.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with 'id_columns', 'key_figure_column', 'week_columns', 'other_columns'\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Detect week columns (columns that match week patterns)\n",
    "    week_patterns = [\n",
    "        r'w\\d+[\\s\\-_]*\\d{4}',  # w20 2025, w20-2025, w20_2025\n",
    "        r'week[\\s\\-_]*\\d+[\\s\\-_]*\\d{4}',  # week 20 2025\n",
    "        r'\\d{4}[\\s\\-_]*w\\d+',  # 2025 w20, 2025-w20\n",
    "    ]\n",
    "    \n",
    "    week_columns = []\n",
    "    for col in columns:\n",
    "        for pattern in week_patterns:\n",
    "            if re.search(pattern, str(col), re.IGNORECASE):\n",
    "                week_columns.append(col)\n",
    "                break\n",
    "    \n",
    "    # Detect key figure column (likely contains categorical data)\n",
    "    key_figure_candidates = []\n",
    "    for col in columns:\n",
    "        if col.lower() in ['key figure', 'key_figure', 'metric', 'measure', 'indicator', 'type']:\n",
    "            key_figure_candidates.append(col)\n",
    "    \n",
    "    # If no obvious key figure column, look for columns with repeated categorical values\n",
    "    if not key_figure_candidates:\n",
    "        for col in columns:\n",
    "            if col not in week_columns:\n",
    "                unique_ratio = len(df[col].unique()) / len(df)\n",
    "                if unique_ratio < 0.1 and df[col].dtype == 'object':  # Less than 10% unique values\n",
    "                    key_figure_candidates.append(col)\n",
    "    \n",
    "    key_figure_column = key_figure_candidates[0] if key_figure_candidates else None\n",
    "    \n",
    "    # ID columns are remaining non-week, non-key-figure columns, but exclude numeric summary columns\n",
    "    excluded_columns = week_columns + ([key_figure_column] if key_figure_column else [])\n",
    "    potential_id_columns = [col for col in columns if col not in excluded_columns]\n",
    "    \n",
    "    # Filter out columns that are likely not ID columns (like 'Total', numeric summaries, etc.)\n",
    "    # Also filter out columns that are mostly NaN\n",
    "    id_columns = []\n",
    "    for col in potential_id_columns:\n",
    "        # Skip columns with names suggesting they're summary/total columns\n",
    "        if col.lower() in ['total', 'sum', 'average', 'avg', 'count', 'grand total']:\n",
    "            continue\n",
    "        \n",
    "        # Skip columns that are mostly NaN (more than 90% NaN)\n",
    "        if df[col].isna().sum() / len(df) > 0.9:\n",
    "            continue\n",
    "            \n",
    "        id_columns.append(col)\n",
    "    \n",
    "    # Other columns (like 'total' etc.)\n",
    "    other_columns = [col for col in potential_id_columns if col not in id_columns]\n",
    "    \n",
    "    return {\n",
    "        'id_columns': id_columns,\n",
    "        'key_figure_column': key_figure_column,\n",
    "        'week_columns': week_columns,\n",
    "        'other_columns': other_columns\n",
    "    }\n",
    "\n",
    "def transform_single_csv(input_file, output_file,current_date_str, config=None):\n",
    "    \"\"\"\n",
    "    Transform a single CSV from wide format to long format.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input CSV file\n",
    "        output_file (str): Path to output CSV file (optional)\n",
    "        config (dict): Manual configuration for columns (optional)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {input_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "    # Step 1: Replace string \"(None)\" with empty string\n",
    "    df.replace(to_replace=r\"\\(None\\)\", value=\"\", regex=True, inplace=True)\n",
    "    \n",
    "    # Step 2: Replace blank strings and NaN with 0\n",
    "    df.replace(to_replace=[\"\", np.nan], value=0, inplace=True)\n",
    "    \n",
    "    # Use provided config or detect columns automatically\n",
    "    if config:\n",
    "        column_info = config\n",
    "    else:\n",
    "        column_info = detect_columns(df)\n",
    "    \n",
    "    print(f\"Detected columns:\")\n",
    "    print(f\"  ID columns: {column_info['id_columns']}\")\n",
    "    print(f\"  Key figure column: {column_info['key_figure_column']}\")\n",
    "    print(f\"  Week columns: {len(column_info['week_columns'])} columns\")\n",
    "    print(f\"  Other columns: {column_info['other_columns']}\")\n",
    "    \n",
    "    # Show which ID columns have data\n",
    "    if column_info['id_columns']:\n",
    "        print(f\"  ID column data availability:\")\n",
    "        for col in column_info['id_columns']:\n",
    "            non_null_pct = (1 - df[col].isna().sum() / len(df)) * 100\n",
    "            print(f\"    {col}: {non_null_pct:.1f}% non-null\")\n",
    "    \n",
    "    # Show sample key figures\n",
    "    if column_info['key_figure_column']:\n",
    "        unique_kf = df[column_info['key_figure_column']].unique()\n",
    "        print(f\"  Key figures ({len(unique_kf)}): {unique_kf[:3]}{'...' if len(unique_kf) > 3 else ''}\")\n",
    "    \n",
    "    # Validate detection\n",
    "    if not column_info['week_columns']:\n",
    "        print(\"ERROR: No week columns detected!\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare for melting\n",
    "    id_vars = column_info['id_columns'] + [column_info['key_figure_column']]\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    try:\n",
    "        melted_df = pd.melt(\n",
    "            df,\n",
    "            id_vars=id_vars,\n",
    "            value_vars=column_info['week_columns'],\n",
    "            var_name='week_year',\n",
    "            value_name='value'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during melting: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert week_year to dates\n",
    "    print(\"Converting week formats to dates...\")\n",
    "    \n",
    "    # Debug: Show sample week formats\n",
    "    sample_weeks = melted_df['week_year'].unique()[:5]\n",
    "    print(f\"Sample week formats: {sample_weeks}\")\n",
    "    \n",
    "    melted_df['date'] = melted_df['week_year'].apply(get_week_start_date)\n",
    "    \n",
    "    # Remove rows where date conversion failed\n",
    "    before_count = len(melted_df)\n",
    "    after_count = len(melted_df)\n",
    "    print(f\"Rows before date filtering: {before_count}\")\n",
    "    print(f\"Rows after date filtering: {after_count}\")\n",
    "    if before_count != after_count:\n",
    "        print(f\"WARNING: {before_count - after_count} rows dropped due to date conversion issues\")\n",
    "        \n",
    "        # Show failed conversions\n",
    "        failed_weeks = df.columns[df.columns.str.contains('w', case=False, na=False)][:3]\n",
    "        print(f\"Sample failed week formats: {failed_weeks}\")\n",
    "    \n",
    "    print(\"Week year conversion done\")\n",
    "    \n",
    "    # Debug: Check data before pivoting\n",
    "    print(f\"Data available for pivoting: {len(melted_df)} rows\")\n",
    "    if len(melted_df) > 0:\n",
    "        print(\"Sample melted data:\")\n",
    "        print(melted_df.head(3).to_string())\n",
    "        print(f\"Unique key figures: {melted_df[column_info['key_figure_column']].unique()}\")\n",
    "        \n",
    "        # Check for NaN values in ID columns\n",
    "        print(\"Checking ID columns for NaN values:\")\n",
    "        for col in column_info['id_columns']:\n",
    "            nan_count = melted_df[col].isna().sum()\n",
    "            print(f\"  {col}: {nan_count} NaN values out of {len(melted_df)}\")\n",
    "        \n",
    "        # If too many NaN values, filter them out or handle differently\n",
    "        id_cols_with_data = [col for col in column_info['id_columns'] \n",
    "                           if melted_df[col].isna().sum() < len(melted_df) * 0.9]\n",
    "        \n",
    "        if not id_cols_with_data:\n",
    "            print(\"WARNING: All ID columns are mostly NaN. Using index-based grouping.\")\n",
    "            # Add a row identifier to prevent empty pivot\n",
    "            melted_df['row_id'] = melted_df.index // len(column_info['week_columns'])\n",
    "            id_cols_with_data = ['row_id']\n",
    "        \n",
    "        print(f\"Using ID columns for pivot: {id_cols_with_data}\")\n",
    "    else:\n",
    "        print(\"ERROR: No data available for pivoting!\")\n",
    "        return None\n",
    "    \n",
    "    # Pivot key figures to columns\n",
    "    try:\n",
    "        print(\"key figure cols\",column_info['key_figure_column'])\n",
    "        pivot_df = melted_df.pivot_table(\n",
    "            index=id_cols_with_data + ['week_year'],\n",
    "            columns=column_info['key_figure_column'],\n",
    "            values='value',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        pivot_df.columns.name = None\n",
    "        \n",
    "        print(f\"Pivot successful. Shape: {pivot_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during pivoting: {e}\")\n",
    "        print(\"Attempting alternative approach...\")\n",
    "        \n",
    "        # Alternative: Don't pivot, just keep as long format\n",
    "        result_df = melted_df.copy()\n",
    "        result_df = result_df.sort_values(id_cols_with_data + ['week_year'])\n",
    "        return result_df\n",
    "    \n",
    "    # Sort the result\n",
    "    sort_columns = id_cols_with_data + ['week_year']\n",
    "    pivot_df = pivot_df.sort_values(sort_columns)\n",
    "\n",
    "    # âœ… Add refresh_date column with current date\n",
    "    #current_date_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    #current_date_str = '21-07-2025'\n",
    "    #pivot_df['refresh_date'] = current_date_str\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTransformation Summary:\")\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    print(f\"  Transformed shape: {pivot_df.shape}\")\n",
    "    print(f\"  Key figures found: {[col for col in pivot_df.columns if col not in id_cols_with_data + ['week_year']]}\")\n",
    "    \n",
    "    # Save if output file specified\n",
    "    if output_file:\n",
    "        try:\n",
    "            # Append to existing CSV if it exists, else create new one\n",
    "            if os.path.exists(output_file):\n",
    "                existing_df = pd.read_csv(output_file)\n",
    "                combined_df = pd.concat([existing_df, pivot_df], ignore_index=True)\n",
    "                combined_df.to_csv(output_file, index=False)\n",
    "                print(f\"  Appended to existing CSV: {output_file}\")\n",
    "            else:\n",
    "                pivot_df.to_csv(output_file, index=False)\n",
    "                print(f\"  Created new CSV: {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving/appending to CSV file: {e}\")\n",
    "\n",
    "    # if output_file:\n",
    "    #     try:\n",
    "    #         pivot_df.to_csv(output_file, index=False)\n",
    "    #         print(f\"  Saved to: {output_file}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error saving file: {e}\")\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preview_file_structure(input_file, num_rows=3):\n",
    "    \"\"\"\n",
    "    Preview file structure to understand the data before transformation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # print(f\"\\n{'='*60}\")\n",
    "        # print(f\"FILE STRUCTURE PREVIEW: {input_file}\")\n",
    "        # print(f\"{'='*60}\")\n",
    "        # print(f\"Shape: {df.shape}\")\n",
    "        # print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # column_info = detect_columns(df)\n",
    "        # print(f\"\\nDetected Structure:\")\n",
    "        # print(f\"  ID columns: {column_info['id_columns']}\")\n",
    "        # print(f\"  Key figure column: {column_info['key_figure_column']}\")\n",
    "        # print(f\"  Week columns: {len(column_info['week_columns'])} columns\")\n",
    "        # print(f\"  Sample week columns: {column_info['week_columns'][:5]}\")\n",
    "        \n",
    "        # if column_info['key_figure_column']:\n",
    "        #     print(f\"  Unique key figures: {df[column_info['key_figure_column']].unique()}\")\n",
    "        \n",
    "        # print(f\"\\nFirst {num_rows} rows:\")\n",
    "        # print(df.head(num_rows).to_string())\n",
    "\n",
    "        print(f'{input_file} is transformed and saved')\n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing {input_file}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage and configuration\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # csvs_name = ['Demand Fulfillment','Review DC', 'Review Component', 'Review Capacity', 'Review Plant', 'Review Vendors','Profit Margin' ]\n",
    "\n",
    "    # for name in csvs_name:\n",
    "    # # Option 1: Transform a single file\n",
    "    #     transform_single_csv(f\"output_w30/{name}.csv\", f\"transformed_w30/{name}_ts.csv\",'21-07-2025')\n",
    "    name = 'Profit Margin'\n",
    "    transform_single_csv(f\"output_w30/{name}.csv\", f\"previous_files_w29_w30/{name}_prev_test.csv\",'21-07-2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0b098-c26f-4779-ba87-0e864aea1693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df194fef-682b-4e2a-ba55-5b252dc675ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
